# ğŸ§  Data Science Project Portfolio: NLP Â· Time Series Â· Neural Networks
## ğŸ“˜ Overview
- This project demonstrates three core applications of modern data science and machine learning techniques across different data types:
        - ğŸ—‚ Natural Language Processing (NLP): Text classification of forum posts.
        - ğŸ“ˆ Time Series Forecasting: Financial trend analysis and forecasting using S&P 500 data.
        - ğŸ¤– Neural Network Modeling: Classification (image-based) and regression (numeric data) tasks using TensorFlow and PyTorch.
- Each section showcases a realistic business use case, key preprocessing steps, model development, evaluation, and insights.
## ğŸ§© Part 1: Natural Language Processing (NLP)
### ğŸ¯ Objective
 Automatically categorize community forum posts (similar to the 20 Newsgroups dataset) to help support teams route queries efficiently.
### ğŸ§° Techniques Used
- Tokenization and text normalization
- Stopword removal
- Lemmatization and stemming
- N-gram extraction
- Word frequency analysis and visualization

### ğŸ“Š Key Insights
- Frequent word analysis revealed domain-specific vocabulary (e.g., â€œspace,â€ â€œNASA,â€ â€œengine,â€ â€œgraphicsâ€).
- N-gram analysis helped capture contextual patterns (e.g., â€œspace shuttle,â€ â€œgraphics cardâ€).
- Comprehensive preprocessing significantly improved the clarity of textual patterns and model input quality.

### ğŸ§  Outcome
Prepared a clean, tokenized corpus suitable for downstream machine learning tasks such as topic classification or sentiment analysis.

## ğŸ“ˆ Part 2: Time Series Forecasting (Financial Data)
### ğŸ¯ Objective
- Analyze and forecast the S&P 500 closing prices to understand market trends and support investment strategy planning.
### ğŸ§° Techniques Used
- Data exploration and visualization
- Stationarity testing using Augmented Dickey-Fuller (ADF)
- Differencing transformation
- Time series decomposition (trend, seasonal, residual)
- ARIMA modeling and diagnostics
- Forecast visualization

### ğŸ§ª Key Findings

| Step                       | Result                                                         |
| -------------------------- | -------------------------------------------------------------- |
| **ADF Test (Original)**    | Non-stationary (p = 0.74)                                      |
| **ADF Test (Differenced)** | Stationary (p â‰ˆ 0.0)                                           |
| **Best ARIMA Model**       | ARIMA(1,1,0) and ARIMA(1,1,1) compared                         |
| **Model Diagnostics**      | Residuals showed no autocorrelation, confirming good model fit |


### ğŸ’¡ Insights
- The original price series showed a strong upward trend and non-stationarity.
- After first differencing, the series became stationary and suitable for ARIMA modeling.
- ARIMA(1,1,1) captured short-term dependencies effectively.
- Residual analysis confirmed model adequacy (Ljungâ€“Box test p > 0.05).

### ğŸ§  Outcome
Built a reliable ARIMA forecasting model capable of predicting near-term market behavior with strong statistical validity.

## ğŸ¤– Part 3: Neural Network Modeling
### ğŸ§© Tasks
- Classification: Recognizing hand-written digits (MNIST-like dataset).
- Regression (optional extension): Predicting numeric outcomes from structured data.
### âš™ï¸ Frameworks
- TensorFlow / Keras for high-level deep learning model development.
- PyTorch for low-level experimentation and model training control.

### ğŸ§  Part 3A â€” TensorFlow/Keras Implementation
 - Architecture:
            - Input: 64 features (flattened 8Ã—8 digit images)
            - Hidden Layers: 128 â†’ 64 (ReLU activation)
            - Output: 10 classes (softmax activation)
- Results:
       - Test Accuracy: 98.6% (ReLU)
       - Validation curves show excellent generalization.
 Activation Function Experimentation:

| Activation  | Test Accuracy | Observations                                                    |
| ----------- | ------------- | --------------------------------------------------------------- |
| **ReLU**    | **0.9861**    | Best performance; fast convergence, minimal vanishing gradients |
| **Sigmoid** | 0.9667        | Slower training; vanishing gradient effects                     |
| **Tanh**    | 0.9833        | Good alternative; smoother convergence but slightly less stable |



### ğŸ“Š Conclusion:
ReLU consistently outperformed other activations, confirming it as the optimal choice for hidden layers in image classification tasks.

### âš¡ Part 3B â€” PyTorch Implementation
- Architecture:
          - Identical to the Keras model for performance comparison.
          - Implemented custom training loop with Adam optimizer and CrossEntropyLoss.
- Results:
       - Test Accuracy: ~98%
        - Smooth loss curve; confirmed model stability and reproducibility across frameworks.

#### ğŸ“Š Conclusion:
Both TensorFlow and PyTorch delivered high-performing models. PyTorch offers more flexibility for experimentation, while TensorFlow excels in ease of deployment.

### ğŸ Overall Conclusions

| Domain              | Key Techniques                       | Core Insight                                                 |
| ------------------- | ------------------------------------ | ------------------------------------------------------------ |
| **NLP**             | Tokenization, Lemmatization, N-grams | Clean text improves feature extraction and topic separation. |
| **Time Series**     | Differencing, ARIMA, Decomposition   | Stationarity is essential for reliable forecasting.          |
| **Neural Networks** | MLPs, Activation Functions           | ReLU leads to faster convergence and higher accuracy.        |

### ğŸ“¦ Technologies Used
- Languages: Python
- Libraries:
      - Data & Visualization: pandas, numpy, matplotlib, seaborn
      - NLP: nltk, sklearn
      - Time Series: statsmodels, yfinance
      - Deep Learning: TensorFlow/Keras, PyTorch

### ğŸ“š Learning Outcomes
- âœ… Mastered preprocessing pipelines for diverse data types
- âœ… Understood statistical vs deep learning modeling paradigms
- âœ… Implemented, tuned, and evaluated machine learning models across domains
- âœ… Gained experience interpreting model outputs and drawing actionable insights

### ğŸš€ Future Enhancements
- Integrate transformer-based NLP models (e.g., BERT).
- Explore LSTM or Prophet for time series forecasting.
- Experiment with CNNs for image data and autoencoders for dimensionality reduction.


#### ğŸ§¾ Author
Faheemunnisa Syeda
     Data Scientist | Machine Learning Enthusiast
         - ğŸ“§ [https://www.linkedin.com/in/faheem-unnisa-s-6270888b/]
         - ğŸ“˜ [https://github.com/syedafaheem7]
